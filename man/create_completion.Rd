% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/create_completion.R
\name{create_completion}
\alias{create_completion}
\title{Create completion}
\usage{
create_completion(
  engine_id,
  prompt = "<|endoftext|>",
  suffix = NULL,
  max_tokens = 16,
  temperature = 1,
  top_p = 1,
  n = 1,
  stream = FALSE,
  logprobs = NULL,
  echo = FALSE,
  stop = NULL,
  presence_penalty = 0,
  frequency_penalty = 0,
  best_of = 1,
  logit_bias = NULL,
  user = NULL,
  openai_api_key = Sys.getenv("OPENAI_API_KEY"),
  openai_organization = NULL
)
}
\arguments{
\item{engine_id}{required; a length one character vector specifying the ID of
the engine to use for this request.}

\item{prompt}{required; defaults to \code{"<|endoftext|>"}; an arbitrary
length character vector specifying prompt(s) to generate completions for.
Note that \code{<|endoftext|>} is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as
if from the beginning of a new document.}

\item{suffix}{optional; defaults to \code{NULL}; a length one
character vector specifying the suffix that comes after a completion of
inserted text.}

\item{max_tokens}{required; defaults to \code{16}; a length one
numeric vector specifying the maximum number of tokens to generate in the
completion. The token count of your prompt plus \code{max_tokens} cannot
exceed the model's context length. Most models have a context length of
\code{2048} tokens (except for the newest models, which support
\code{4096}).}

\item{temperature}{required; defaults to \code{1}; a length one numeric
vector between \code{0} and \code{2} specifying what sampling temperature to
use. Higher values means the model will take more risks. Try \code{0.9} for
more creative applications, and \code{0} (argmax sampling) for ones with a
well-defined answer. We generally recommend altering (i.e., setting the value
different from \code{1}) this or \code{top_p} but not both.}

\item{top_p}{required; defaults to \code{1}; a length one numeric
vector between \code{0} and \code{2} specifying an alternative to sampling
with temperature, called nucleus sampling, where the model considers the
results of the tokens with \code{top_p} probability mass. So \code{0.1} means
only the tokens comprising the top 10\% probability mass are considered. We
generally recommend altering (i.e., setting the value different from
\code{1}) this or \code{temperature} but not both.}

\item{n}{required; defaults to \code{1}; a length one numeric vector
specifying how many completions to generate for each prompt. Note: Because
this parameter generates many completions, it can quickly consume your token
quota. Use carefully and ensure that you have reasonable settings for
\code{max_tokens} and \code{stop}.}

\item{stream}{required; defaults to \code{FALSE}; a length one logical vector
; currently is not implemented.}

\item{openai_api_key}{required; defaults to
\code{Sys.getenv("OPENAI_API_KEY")} (i.e., the value is retrieved from the
\code{.Renviron} file); a length one character vector containing OpenAI API
key.}

\item{openai_organization}{optional; defaults to \code{NULL}; a length one
character vector specifying OpenAI organization.}
}
\value{
Returns a list, an element of which is a data frame containing
information about engines.
}
\description{
Creates a new completion for the provided prompt and parameters. See
\href{https://beta.openai.com/docs/api-reference/completions/create}{this page}
 for details.
}
\examples{
\dontrun{
list_engines()
}
}
\seealso{
Other engine functions: 
\code{\link{list_engines}()},
\code{\link{retrieve_engine}()}
}
\concept{engine functions}
